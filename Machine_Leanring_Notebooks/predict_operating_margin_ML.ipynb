{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retailer_ID</th>\n",
       "      <th>Invoice_Date</th>\n",
       "      <th>Region_ID</th>\n",
       "      <th>State_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Price_per_unit</th>\n",
       "      <th>Units_sold</th>\n",
       "      <th>Operating_margin</th>\n",
       "      <th>Sales_method</th>\n",
       "      <th>Total_sales</th>\n",
       "      <th>Operating_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.50</td>\n",
       "      <td>In-store</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>In-store</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.35</td>\n",
       "      <td>In-store</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD4</td>\n",
       "      <td>45.0</td>\n",
       "      <td>850</td>\n",
       "      <td>0.35</td>\n",
       "      <td>In-store</td>\n",
       "      <td>38250.0</td>\n",
       "      <td>13387.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>900</td>\n",
       "      <td>0.30</td>\n",
       "      <td>In-store</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>16200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Retailer_ID Invoice_Date Region_ID State_ID Product_ID  Price_per_unit  \\\n",
       "0        RTL1   2020-01-01       RG1      ST1        PD1            50.0   \n",
       "1        RTL1   2020-01-02       RG1      ST1        PD2            50.0   \n",
       "2        RTL1   2020-01-03       RG1      ST1        PD3            40.0   \n",
       "3        RTL1   2020-01-04       RG1      ST1        PD4            45.0   \n",
       "4        RTL1   2020-01-05       RG1      ST1        PD5            60.0   \n",
       "\n",
       "   Units_sold  Operating_margin Sales_method  Total_sales  Operating_profit  \n",
       "0        1200              0.50     In-store      60000.0           30000.0  \n",
       "1        1000              0.30     In-store      50000.0           15000.0  \n",
       "2        1000              0.35     In-store      40000.0           14000.0  \n",
       "3         850              0.35     In-store      38250.0           13387.5  \n",
       "4         900              0.30     In-store      54000.0           16200.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import sales data\n",
    "adidas_sales_df = pd.read_csv('../adidas_sales.csv')\n",
    "adidas_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Retailer_ID          object\n",
       "Invoice_Date         object\n",
       "Region_ID            object\n",
       "State_ID             object\n",
       "Product_ID           object\n",
       "Price_per_unit      float64\n",
       "Units_sold            int64\n",
       "Operating_margin    float64\n",
       "Sales_method         object\n",
       "Total_sales         float64\n",
       "Operating_profit    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adidas_sales_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date to datetime\n",
    "adidas_sales_df['Invoice_Date'] = pd.to_datetime(adidas_sales_df['Invoice_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35    1309\n",
       "0.40    1003\n",
       "0.30     722\n",
       "0.50     610\n",
       "0.45     364\n",
       "        ... \n",
       "0.48       1\n",
       "0.27       1\n",
       "0.17       1\n",
       "0.65       1\n",
       "0.17       1\n",
       "Name: Operating_margin, Length: 110, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adidas_sales_df['Operating_margin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retailer_ID</th>\n",
       "      <th>Invoice_Date</th>\n",
       "      <th>Region_ID</th>\n",
       "      <th>State_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Price_per_unit</th>\n",
       "      <th>Units_sold</th>\n",
       "      <th>Operating_margin</th>\n",
       "      <th>Sales_method</th>\n",
       "      <th>Total_sales</th>\n",
       "      <th>Operating_profit</th>\n",
       "      <th>Operating Margin Equal to or Above 0.35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.50</td>\n",
       "      <td>In-store</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>In-store</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.35</td>\n",
       "      <td>In-store</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD4</td>\n",
       "      <td>45.0</td>\n",
       "      <td>850</td>\n",
       "      <td>0.35</td>\n",
       "      <td>In-store</td>\n",
       "      <td>38250.0</td>\n",
       "      <td>13387.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RTL1</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>RG1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>PD5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>900</td>\n",
       "      <td>0.30</td>\n",
       "      <td>In-store</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Retailer_ID Invoice_Date Region_ID State_ID Product_ID  Price_per_unit  \\\n",
       "0        RTL1   2020-01-01       RG1      ST1        PD1            50.0   \n",
       "1        RTL1   2020-01-02       RG1      ST1        PD2            50.0   \n",
       "2        RTL1   2020-01-03       RG1      ST1        PD3            40.0   \n",
       "3        RTL1   2020-01-04       RG1      ST1        PD4            45.0   \n",
       "4        RTL1   2020-01-05       RG1      ST1        PD5            60.0   \n",
       "\n",
       "   Units_sold  Operating_margin Sales_method  Total_sales  Operating_profit  \\\n",
       "0        1200              0.50     In-store      60000.0           30000.0   \n",
       "1        1000              0.30     In-store      50000.0           15000.0   \n",
       "2        1000              0.35     In-store      40000.0           14000.0   \n",
       "3         850              0.35     In-store      38250.0           13387.5   \n",
       "4         900              0.30     In-store      54000.0           16200.0   \n",
       "\n",
       "   Operating Margin Equal to or Above 0.35  \n",
       "0                                        1  \n",
       "1                                        0  \n",
       "2                                        1  \n",
       "3                                        1  \n",
       "4                                        0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add binary column for operating margin above 0.35\n",
    "adidas_sales_df['Operating Margin Equal to or Above 0.35'] = adidas_sales_df['Operating_margin'] >= 0.35\n",
    "\n",
    "#Convert boolean to int\n",
    "adidas_sales_df['Operating Margin Equal to or Above 0.35'] = adidas_sales_df['Operating Margin Equal to or Above 0.35'].astype(int)\n",
    "\n",
    "adidas_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define feature set\n",
    "X = adidas_sales_df.drop(['Operating_margin', 'Operating Margin Equal to or Above 0.35'], axis=1)\n",
    "\n",
    "#Define target set\n",
    "y = adidas_sales_df['Operating Margin Equal to or Above 0.35']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dummy variables for categorical data\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Invoice_Date', 'Price_per_unit', 'Units_sold', 'Total_sales',\n",
       "       'Operating_profit', 'Retailer_ID_RTL1', 'Retailer_ID_RTL2',\n",
       "       'Retailer_ID_RTL3', 'Retailer_ID_RTL4', 'Retailer_ID_RTL5',\n",
       "       'Retailer_ID_RTL6', 'Region_ID_RG1', 'Region_ID_RG2', 'Region_ID_RG3',\n",
       "       'Region_ID_RG4', 'Region_ID_RG5', 'State_ID_ST1', 'State_ID_ST10',\n",
       "       'State_ID_ST11', 'State_ID_ST12', 'State_ID_ST13', 'State_ID_ST14',\n",
       "       'State_ID_ST15', 'State_ID_ST16', 'State_ID_ST17', 'State_ID_ST18',\n",
       "       'State_ID_ST19', 'State_ID_ST2', 'State_ID_ST20', 'State_ID_ST21',\n",
       "       'State_ID_ST22', 'State_ID_ST23', 'State_ID_ST24', 'State_ID_ST25',\n",
       "       'State_ID_ST26', 'State_ID_ST27', 'State_ID_ST28', 'State_ID_ST29',\n",
       "       'State_ID_ST3', 'State_ID_ST30', 'State_ID_ST31', 'State_ID_ST32',\n",
       "       'State_ID_ST33', 'State_ID_ST34', 'State_ID_ST35', 'State_ID_ST36',\n",
       "       'State_ID_ST37', 'State_ID_ST38', 'State_ID_ST39', 'State_ID_ST4',\n",
       "       'State_ID_ST40', 'State_ID_ST41', 'State_ID_ST42', 'State_ID_ST43',\n",
       "       'State_ID_ST44', 'State_ID_ST45', 'State_ID_ST46', 'State_ID_ST47',\n",
       "       'State_ID_ST48', 'State_ID_ST49', 'State_ID_ST5', 'State_ID_ST50',\n",
       "       'State_ID_ST6', 'State_ID_ST7', 'State_ID_ST8', 'State_ID_ST9',\n",
       "       'Product_ID_PD1', 'Product_ID_PD2', 'Product_ID_PD3', 'Product_ID_PD4',\n",
       "       'Product_ID_PD5', 'Product_ID_PD6', 'Sales_method_In-store',\n",
       "       'Sales_method_Online', 'Sales_method_Outlet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fit the StandardScaler with the training data\n",
    "X_scaler = scaler.fit(X_train.drop(['Invoice_Date'], axis=1))\n",
    "\n",
    "#Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train.drop(['Invoice_Date'], axis=1))\n",
    "X_test_scaled = X_scaler.transform(X_test.drop(['Invoice_Date'], axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create decision tree classifier instance\n",
    "dtr = tree.DecisionTreeClassifier()\n",
    "\n",
    "#Fit the model\n",
    "dtr = dtr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Make predictions\n",
    "dtr_predictions = dtr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>340</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>52</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          340           55\n",
       "Actual 1           52         1965"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "dtr_cm = confusion_matrix(y_test, dtr_predictions)\n",
    "\n",
    "#Create a DataFrame from the confusion matrix\n",
    "dtr_cm_df = pd.DataFrame(dtr_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "dtr_cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9556384742951907"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the accuracy score\n",
    "dtr_acc_score = accuracy_score(y_test, dtr_predictions)\n",
    "dtr_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86       395\n",
      "           1       0.97      0.97      0.97      2017\n",
      "\n",
      "    accuracy                           0.96      2412\n",
      "   macro avg       0.92      0.92      0.92      2412\n",
      "weighted avg       0.96      0.96      0.96      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the classification report\n",
    "print(classification_report(y_test, dtr_predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Decision Tree Regression model using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=4,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                         'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18,\n",
       "                                               20]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find optimal parameters using GridSearchCV\n",
    "dtr_grid = {'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n",
    "            'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "#Create the GridSearchCV object\n",
    "CV_dtr = GridSearchCV(estimator=dtr, param_grid=dtr_grid, cv=5, n_jobs=4)\n",
    "CV_dtr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 20,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the optimal parameters\n",
    "CV_dtr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 1, Min Samples Split: 2, Accuracy Score: 0.9589552238805971\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 1, Min Samples Split: 4, Accuracy Score: 0.9564676616915423\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 1, Min Samples Split: 6, Accuracy Score: 0.9564676616915423\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 1, Min Samples Split: 8, Accuracy Score: 0.9564676616915423\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 2, Accuracy Score: 0.9601990049751243\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 4, Accuracy Score: 0.9581260364842454\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 6, Accuracy Score: 0.9577114427860697\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 8, Accuracy Score: 0.956882255389718\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 10, Accuracy Score: 0.9581260364842454\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 12, Accuracy Score: 0.9556384742951907\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 14, Accuracy Score: 0.9581260364842454\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 16, Accuracy Score: 0.9556384742951907\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 18, Accuracy Score: 0.956882255389718\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 2, Min Samples Split: 20, Accuracy Score: 0.9577114427860697\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 3, Min Samples Split: 4, Accuracy Score: 0.9560530679933665\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 3, Min Samples Split: 6, Accuracy Score: 0.9556384742951907\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 3, Min Samples Split: 8, Accuracy Score: 0.9577114427860697\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 3, Min Samples Split: 12, Accuracy Score: 0.9556384742951907\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 3, Min Samples Split: 14, Accuracy Score: 0.9572968490878938\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 3, Min Samples Split: 16, Accuracy Score: 0.9572968490878938\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 2, Accuracy Score: 0.9589552238805971\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 4, Accuracy Score: 0.9581260364842454\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 6, Accuracy Score: 0.9589552238805971\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 8, Accuracy Score: 0.9593698175787728\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 10, Accuracy Score: 0.956882255389718\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 12, Accuracy Score: 0.9585406301824212\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 16, Accuracy Score: 0.956882255389718\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 18, Accuracy Score: 0.9564676616915423\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 4, Min Samples Split: 20, Accuracy Score: 0.9585406301824212\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 5, Min Samples Split: 14, Accuracy Score: 0.9556384742951907\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 5, Min Samples Split: 20, Accuracy Score: 0.956882255389718\n",
      "Criterion: entropy, Max Depth: 20, Min Samples Leaf: 6, Min Samples Split: 20, Accuracy Score: 0.9560530679933665\n"
     ]
    }
   ],
   "source": [
    "#Use the past accuracy score to find the best parameters\n",
    "opt_dtr_acc_score_loop = 0.9556\n",
    "\n",
    "#For loop to find the best parameters\n",
    "for c in dtr_grid['criterion']:\n",
    "    for d in dtr_grid['max_depth']:\n",
    "        for msl in dtr_grid['min_samples_leaf']:\n",
    "            for mss in dtr_grid['min_samples_split']:\n",
    "                dtr = tree.DecisionTreeClassifier(criterion=c, max_depth=d, min_samples_leaf=msl, min_samples_split=mss)\n",
    "                dtr = dtr.fit(X_train_scaled, y_train)\n",
    "                dtr_predictions = dtr.predict(X_test_scaled)\n",
    "                dtr_acc_score_in_loop = accuracy_score(y_test, dtr_predictions)\n",
    "                \n",
    "                #If the accuracy score is better than the previous one, save the parameters\n",
    "                if dtr_acc_score_in_loop > opt_dtr_acc_score_loop:\n",
    "                    print(f'Criterion: {c}, Max Depth: {d}, Min Samples Leaf: {msl}, Min Samples Split: {mss}, Accuracy Score: {dtr_acc_score_in_loop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loop accuracy score was not better than the GridSearchCV accuracy score\n",
    "\n",
    "#Create decision tree classifier instance\n",
    "opt_dtr = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=1, min_samples_split=2)\n",
    "\n",
    "#Fit the model\n",
    "opt_dtr = opt_dtr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Make predictions\n",
    "opt_dtr_predictions = opt_dtr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized DTR Confusion Matrix:\n",
      "           Predicted 0  Predicted 1\n",
      "Actual 0          334           61\n",
      "Actual 1           39         1978\n",
      "Original DTR Confusion Matrix:\n",
      "           Predicted 0  Predicted 1\n",
      "Actual 0          340           55\n",
      "Actual 1           52         1965\n"
     ]
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "opt_dtr_cm = confusion_matrix(y_test, opt_dtr_predictions)\n",
    "\n",
    "#Create a DataFrame from the confusion matrix\n",
    "opt_dtr_cm_df = pd.DataFrame(opt_dtr_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "print(f'Optimized DTR Confusion Matrix:\\n', opt_dtr_cm_df)\n",
    "print(f'Original DTR Confusion Matrix:\\n', dtr_cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized DTR Accuracy Score: 0.9585406301824212\n",
      "Original DTR Accuracy Score: 0.9556384742951907\n"
     ]
    }
   ],
   "source": [
    "#Calculate the accuracy score\n",
    "opt_dtr_acc_score = accuracy_score(y_test, opt_dtr_predictions)\n",
    "\n",
    "print(f'Optimized DTR Accuracy Score: {opt_dtr_acc_score}')\n",
    "print(f'Original DTR Accuracy Score: {dtr_acc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized DTR Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87       395\n",
      "           1       0.97      0.98      0.98      2017\n",
      "\n",
      "    accuracy                           0.96      2412\n",
      "   macro avg       0.93      0.91      0.92      2412\n",
      "weighted avg       0.96      0.96      0.96      2412\n",
      "\n",
      "Original DTR Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86       395\n",
      "           1       0.97      0.97      0.97      2017\n",
      "\n",
      "    accuracy                           0.96      2412\n",
      "   macro avg       0.92      0.92      0.92      2412\n",
      "weighted avg       0.96      0.96      0.96      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the classification report\n",
    "print(f'Optimized DTR Report:\\n', classification_report(y_test, opt_dtr_predictions))\n",
    "print(f'Original DTR Report:\\n', classification_report(y_test, dtr_predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a random forest classifier\n",
    "rfr = RandomForestClassifier(n_estimators=128, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "rfr = rfr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "rfr_predictions = rfr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>328</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>24</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          328           67\n",
       "Actual 1           24         1993"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "rfr_cm = confusion_matrix(y_test, rfr_predictions)\n",
    "\n",
    "#Create a DataFrame from the confusion matrix\n",
    "rfr_cm_df = pd.DataFrame(rfr_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "rfr_cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622719734660033"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the accuracy score\n",
    "rfr_acc_score = accuracy_score(y_test, rfr_predictions)\n",
    "rfr_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       395\n",
      "           1       0.97      0.99      0.98      2017\n",
      "\n",
      "    accuracy                           0.96      2412\n",
      "   macro avg       0.95      0.91      0.93      2412\n",
      "weighted avg       0.96      0.96      0.96      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the classification report\n",
    "print(classification_report(y_test, rfr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.15182349810194812, 'Total_sales'),\n",
       " (0.1166484709025738, 'Units_sold'),\n",
       " (0.09271771142109246, 'Price_per_unit'),\n",
       " (0.08824188673452729, 'Invoice_Date'),\n",
       " (0.052018284757655485, 'Sales_method_In-store'),\n",
       " (0.035489596532471315, 'Product_ID_PD5'),\n",
       " (0.025105200956083863, 'Product_ID_PD4'),\n",
       " (0.024887412679273607, 'Product_ID_PD6'),\n",
       " (0.02407410711942072, 'Product_ID_PD1'),\n",
       " (0.020070531804078546, 'Sales_method_Online'),\n",
       " (0.01990324474205491, 'Region_ID_RG2'),\n",
       " (0.019737091458378354, 'State_ID_ST47'),\n",
       " (0.018166818963153072, 'State_ID_ST9'),\n",
       " (0.017907019936051805, 'Product_ID_PD3'),\n",
       " (0.01789342356537714, 'Product_ID_PD2'),\n",
       " (0.016444274771791206, 'Region_ID_RG1'),\n",
       " (0.01494218140624257, 'Retailer_ID_RTL2'),\n",
       " (0.014774038888097845, 'State_ID_ST7'),\n",
       " (0.014142105923244405, 'State_ID_ST16'),\n",
       " (0.013473564653812678, 'Operating_profit'),\n",
       " (0.011430618380322914, 'Retailer_ID_RTL6'),\n",
       " (0.010412816425545948, 'Retailer_ID_RTL3'),\n",
       " (0.010373597200210997, 'State_ID_ST8'),\n",
       " (0.010178674670904494, 'State_ID_ST29'),\n",
       " (0.00997655835124559, 'Retailer_ID_RTL1'),\n",
       " (0.008163208827235821, 'Retailer_ID_RTL5'),\n",
       " (0.00766960011892011, 'Region_ID_RG3'),\n",
       " (0.006776805420675176, 'Retailer_ID_RTL4'),\n",
       " (0.006179482897317677, 'State_ID_ST5'),\n",
       " (0.005917233735806735, 'State_ID_ST50'),\n",
       " (0.005750437544357576, 'State_ID_ST15'),\n",
       " (0.005733422150699692, 'Region_ID_RG4'),\n",
       " (0.005596976760697211, 'State_ID_ST36'),\n",
       " (0.005065826253260652, 'State_ID_ST38'),\n",
       " (0.004869893075301476, 'Region_ID_RG5'),\n",
       " (0.004722675921294615, 'State_ID_ST48'),\n",
       " (0.0045257702002606415, 'State_ID_ST1'),\n",
       " (0.004438324126031825, 'State_ID_ST21'),\n",
       " (0.004346172004104674, 'State_ID_ST4'),\n",
       " (0.004210451432767935, 'State_ID_ST22'),\n",
       " (0.0038752783703885504, 'State_ID_ST14'),\n",
       " (0.003817781914991458, 'State_ID_ST45'),\n",
       " (0.00370557573492389, 'State_ID_ST25'),\n",
       " (0.0036115592442926775, 'State_ID_ST37'),\n",
       " (0.0031528959689893867, 'State_ID_ST24'),\n",
       " (0.0031063716189010096, 'State_ID_ST27'),\n",
       " (0.0031026243728183084, 'State_ID_ST49'),\n",
       " (0.0029726259429211717, 'State_ID_ST18'),\n",
       " (0.002932222497095693, 'State_ID_ST6'),\n",
       " (0.002530975740734045, 'State_ID_ST46'),\n",
       " (0.002516259108614593, 'State_ID_ST40'),\n",
       " (0.0024626225544449663, 'State_ID_ST19'),\n",
       " (0.0023456462934589428, 'State_ID_ST26'),\n",
       " (0.0022478139605629785, 'State_ID_ST23'),\n",
       " (0.0020787109396779428, 'State_ID_ST20'),\n",
       " (0.0017892540686916217, 'State_ID_ST28'),\n",
       " (0.0017786351010355805, 'State_ID_ST39'),\n",
       " (0.0017188232229424186, 'State_ID_ST42'),\n",
       " (0.0016623884906328142, 'State_ID_ST17'),\n",
       " (0.0015747762049398775, 'State_ID_ST10'),\n",
       " (0.0015551572550025902, 'State_ID_ST12'),\n",
       " (0.001258880375527876, 'State_ID_ST13'),\n",
       " (0.0012439441259497446, 'State_ID_ST43'),\n",
       " (0.001157021454565324, 'State_ID_ST41'),\n",
       " (0.0011499956736065382, 'State_ID_ST2'),\n",
       " (0.001091230427716844, 'State_ID_ST44'),\n",
       " (0.001081730775455219, 'State_ID_ST11'),\n",
       " (0.0007950891651046518, 'State_ID_ST32'),\n",
       " (0.0007442096859871857, 'State_ID_ST3'),\n",
       " (0.000602335593048767, 'State_ID_ST35'),\n",
       " (0.0005846670678681421, 'State_ID_ST34'),\n",
       " (0.0003722752849585196, 'State_ID_ST33'),\n",
       " (0.000365238488150385, 'State_ID_ST30'),\n",
       " (0.00021637246170951504, 'State_ID_ST31')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate feature importance in the Random Forest model\n",
    "importances = rfr.feature_importances_\n",
    "sorted(zip(rfr.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Random Forest Regression model using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=4,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [2, 6, 10, 14, 18],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 3, 5, 7, 9],\n",
       "                         'min_samples_split': [2, 6, 10, 14],\n",
       "                         'n_estimators': [100, 300, 500, 700]})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find optimal parameters using GridSearchCV\n",
    "rfr_grid = {'criterion': ['gini', 'entropy'],\n",
    "            'n_estimators': [100, 300, 500, 700],\n",
    "            'max_depth': [2, 6, 10, 14, 18],\n",
    "            'min_samples_leaf': [1, 3, 5, 7, 9],\n",
    "            'min_samples_split': [2, 6, 10, 14],\n",
    "            'max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "#Create the GridSearchCV object\n",
    "CV_rfr = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rfr_grid, cv=5, n_jobs=4)\n",
    "CV_rfr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 18,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 700}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the optimal parameters\n",
    "CV_rfr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the past accuracy score to find the best parameters\n",
    "rfr_acc_score_loop = 0.9622\n",
    "\n",
    "#For loop to find the best parameters\n",
    "for c in rfr_grid['criterion']:\n",
    "    for n in rfr_grid['n_estimators']:\n",
    "        for d in rfr_grid['max_depth']:\n",
    "            for msl in rfr_grid['min_samples_leaf']:\n",
    "                for mss in rfr_grid['min_samples_split']:\n",
    "                    for mf in rfr_grid['max_features']:\n",
    "                        rfr = RandomForestClassifier(criterion=c, n_estimators=n, max_depth=d, min_samples_leaf=msl, min_samples_split=mss, max_features=mf)\n",
    "                        rfr = rfr.fit(X_train_scaled, y_train)\n",
    "                        rfr_predictions = rfr.predict(X_test_scaled)\n",
    "                        rfr_acc_score_in_loop = accuracy_score(y_test, rfr_predictions)\n",
    "                \n",
    "                        #If the accuracy score is better than the previous one, save the parameters\n",
    "                        if rfr_acc_score_in_loop > rfr_acc_score_loop:\n",
    "                            rfr_acc_score_loop = rfr_acc_score_in_loop\n",
    "                            print(f'Criterion: {c}, N Estimators: {n}, Max Depth: {d}, Min Samples Leaf: {msl}, Min Samples Split: {mss}, Max Features: {mf}, Accuracy Score: {rfr_acc_score_in_loop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Somehow neither the for loop nor the GridSeachCV were able to produce a better accuracy score than the original model\n",
    "\n",
    "#Create random forest classifier instance\n",
    "opt_rfr = RandomForestClassifier(criterion='gini', max_depth=18, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=700)\n",
    "\n",
    "#Fit the model\n",
    "opt_rfr = opt_rfr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Make predictions\n",
    "opt_rfr_predictions = opt_rfr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RFR Confusion Matrix:\n",
      "           Predicted 0  Predicted 1\n",
      "Actual 0          311           84\n",
      "Actual 1           17         2000\n",
      "Original RFR Confusion Matrix:\n",
      "           Predicted 0  Predicted 1\n",
      "Actual 0          328           67\n",
      "Actual 1           24         1993\n"
     ]
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "opt_rfr_cm = confusion_matrix(y_test, opt_rfr_predictions)\n",
    "\n",
    "#Create a DataFrame from the confusion matrix\n",
    "opt_rfr_cm_df = pd.DataFrame(opt_rfr_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "print(f'Optimized RFR Confusion Matrix:\\n', opt_rfr_cm_df)\n",
    "print(f'Original RFR Confusion Matrix:\\n', rfr_cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RFR Accuracy Score: 0.9581260364842454\n",
      "Original RFR Accuracy Score: 0.9622719734660033\n"
     ]
    }
   ],
   "source": [
    "#Calculate the accuracy score\n",
    "opt_rfr_acc_score = accuracy_score(y_test, opt_rfr_predictions)\n",
    "\n",
    "print(f'Optimized RFR Accuracy Score: {opt_rfr_acc_score}')\n",
    "print(f'Original RFR Accuracy Score: {rfr_acc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RFR Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       395\n",
      "           1       0.96      0.99      0.98      2017\n",
      "\n",
      "    accuracy                           0.96      2412\n",
      "   macro avg       0.95      0.89      0.92      2412\n",
      "weighted avg       0.96      0.96      0.96      2412\n",
      "\n",
      "Original RFR Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       395\n",
      "           1       0.97      0.99      0.98      2017\n",
      "\n",
      "    accuracy                           0.96      2412\n",
      "   macro avg       0.95      0.91      0.93      2412\n",
      "weighted avg       0.96      0.96      0.96      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the classification report\n",
    "print(f'Optimized RFR Report:\\n', classification_report(y_test, opt_rfr_predictions))\n",
    "print(f'Original RFR Report:\\n', classification_report(y_test, rfr_predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use best learning rate to create model\n",
    "gbr = GradientBoostingClassifier()\n",
    "\n",
    "#Fit the model\n",
    "gbr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Make predictions\n",
    "gbr_predictions = gbr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>309</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>12</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          309           86\n",
       "Actual 1           12         2005"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "gbr_cm = confusion_matrix(y_test, gbr_predictions)\n",
    "\n",
    "#Create a DataFrame from the confusion matrix\n",
    "gbr_cm_df = pd.DataFrame(gbr_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "gbr_cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9593698175787728"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the accuracy score\n",
    "gb_acc_score = accuracy_score(y_test, gbr_predictions)\n",
    "gb_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86       395\n",
      "           1       0.96      0.99      0.98      2017\n",
      "\n",
      "    accuracy                           0.96      2412\n",
      "   macro avg       0.96      0.89      0.92      2412\n",
      "weighted avg       0.96      0.96      0.96      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the classification report\n",
    "print(classification_report(y_test, gbr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3200396670255088, 'Total_sales'),\n",
       " (0.2659822800880565, 'Units_sold'),\n",
       " (0.1454631082923365, 'Sales_method_In-store'),\n",
       " (0.03755674475518907, 'Region_ID_RG1'),\n",
       " (0.03393870838408824, 'Product_ID_PD5'),\n",
       " (0.03281949929892853, 'Invoice_Date'),\n",
       " (0.031820044650584474, 'State_ID_ST47'),\n",
       " (0.015920662506392222, 'State_ID_ST7'),\n",
       " (0.015360755037334322, 'Region_ID_RG2'),\n",
       " (0.015237791515311088, 'State_ID_ST9'),\n",
       " (0.00830339475938698, 'State_ID_ST16'),\n",
       " (0.00771445357565799, 'State_ID_ST36'),\n",
       " (0.0071205628629160585, 'Price_per_unit'),\n",
       " (0.006065454163380283, 'Retailer_ID_RTL1'),\n",
       " (0.006058305505380121, 'State_ID_ST8'),\n",
       " (0.005438930642948589, 'State_ID_ST29'),\n",
       " (0.005144186363869738, 'Product_ID_PD4'),\n",
       " (0.0035955057973683082, 'Product_ID_PD6'),\n",
       " (0.003528838419320471, 'State_ID_ST21'),\n",
       " (0.00349760689487835, 'State_ID_ST48'),\n",
       " (0.003397811685745328, 'Retailer_ID_RTL2'),\n",
       " (0.002959050691623667, 'State_ID_ST27'),\n",
       " (0.00283096923834162, 'State_ID_ST22'),\n",
       " (0.002773269107873658, 'Retailer_ID_RTL6'),\n",
       " (0.0025362678469267423, 'State_ID_ST38'),\n",
       " (0.0022782151038236807, 'Sales_method_Online'),\n",
       " (0.002212581874341903, 'State_ID_ST28'),\n",
       " (0.001654029632924866, 'State_ID_ST25'),\n",
       " (0.0016394341073146493, 'State_ID_ST19'),\n",
       " (0.0015397700711212562, 'State_ID_ST12'),\n",
       " (0.001328999867231664, 'Product_ID_PD3'),\n",
       " (0.001113013164745302, 'State_ID_ST37'),\n",
       " (0.0008841359385618751, 'State_ID_ST4'),\n",
       " (0.0008344380056377279, 'State_ID_ST5'),\n",
       " (0.0005176275887734072, 'State_ID_ST18'),\n",
       " (0.00042885940621597597, 'State_ID_ST20'),\n",
       " (0.0003142387582538401, 'State_ID_ST50'),\n",
       " (0.0001000634427430676, 'State_ID_ST45'),\n",
       " (5.072392896335659e-05, 'State_ID_ST26'),\n",
       " (0.0, 'State_ID_ST6'),\n",
       " (0.0, 'State_ID_ST49'),\n",
       " (0.0, 'State_ID_ST46'),\n",
       " (0.0, 'State_ID_ST44'),\n",
       " (0.0, 'State_ID_ST43'),\n",
       " (0.0, 'State_ID_ST42'),\n",
       " (0.0, 'State_ID_ST41'),\n",
       " (0.0, 'State_ID_ST40'),\n",
       " (0.0, 'State_ID_ST39'),\n",
       " (0.0, 'State_ID_ST35'),\n",
       " (0.0, 'State_ID_ST34'),\n",
       " (0.0, 'State_ID_ST33'),\n",
       " (0.0, 'State_ID_ST32'),\n",
       " (0.0, 'State_ID_ST31'),\n",
       " (0.0, 'State_ID_ST30'),\n",
       " (0.0, 'State_ID_ST3'),\n",
       " (0.0, 'State_ID_ST24'),\n",
       " (0.0, 'State_ID_ST23'),\n",
       " (0.0, 'State_ID_ST2'),\n",
       " (0.0, 'State_ID_ST17'),\n",
       " (0.0, 'State_ID_ST15'),\n",
       " (0.0, 'State_ID_ST14'),\n",
       " (0.0, 'State_ID_ST13'),\n",
       " (0.0, 'State_ID_ST11'),\n",
       " (0.0, 'State_ID_ST10'),\n",
       " (0.0, 'State_ID_ST1'),\n",
       " (0.0, 'Retailer_ID_RTL5'),\n",
       " (0.0, 'Retailer_ID_RTL4'),\n",
       " (0.0, 'Retailer_ID_RTL3'),\n",
       " (0.0, 'Region_ID_RG5'),\n",
       " (0.0, 'Region_ID_RG4'),\n",
       " (0.0, 'Region_ID_RG3'),\n",
       " (0.0, 'Product_ID_PD2'),\n",
       " (0.0, 'Product_ID_PD1'),\n",
       " (0.0, 'Operating_profit')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate feature importance in the Gradient Boosting model\n",
    "gb_importances = gbr.feature_importances_\n",
    "sorted(zip(gbr.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Gradient Boosting Regression model using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have tried to use GridSearchCV three times with different parameters and it has taken far too long to run. I have commented out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\Data Analytics\\Analysis Projects\\Final Project\\Final-Project\\Machine_Leanring_Notebooks\\predict_operating_margin_ML.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Create the GridSearchCV object\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y105sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m CV_gbr \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mGradientBoostingClassifier(), param_grid\u001b[39m=\u001b[39mgbr_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y105sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m CV_gbr\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train)\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m         clone(base_estimator),\n\u001b[0;32m    841\u001b[0m         X,\n\u001b[0;32m    842\u001b[0m         y,\n\u001b[0;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    849\u001b[0m     )\n\u001b[0;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    852\u001b[0m     )\n\u001b[0;32m    853\u001b[0m )\n\u001b[0;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1061\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1062\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 938\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    939\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Find optimal parameters using GridSearchCV\n",
    "gbr_grid = {'n_estimators': [100, 300, 500, 700],\n",
    "            'max_depth': [2, 6, 10, 14],\n",
    "            'min_samples_leaf': [1, 3, 5, 7],\n",
    "            'min_samples_split': [2, 6, 10, 14],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'learning_rate': [0.01, 0.1, 0.5, 0.75]}\n",
    "\n",
    "#Create the GridSearchCV object\n",
    "CV_gbr = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=gbr_grid, cv=5, n_jobs=4)\n",
    "CV_gbr.fit(X_train_scaled, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, max_depth: 2, learning_rate: 0.5, min_samples_leaf: 1, min_samples_split: 2, max_features: auto\n",
      "Accuracy Score: 0.9825870646766169\n",
      "n_estimators: 100, max_depth: 2, learning_rate: 0.5, min_samples_leaf: 3, min_samples_split: 2, max_features: auto\n",
      "Accuracy Score: 0.9871475953565506\n",
      "n_estimators: 100, max_depth: 2, learning_rate: 0.75, min_samples_leaf: 1, min_samples_split: 2, max_features: auto\n",
      "Accuracy Score: 0.9888059701492538\n",
      "n_estimators: 100, max_depth: 6, learning_rate: 0.5, min_samples_leaf: 1, min_samples_split: 2, max_features: auto\n",
      "Accuracy Score: 0.9917081260364843\n",
      "n_estimators: 100, max_depth: 10, learning_rate: 0.5, min_samples_leaf: 1, min_samples_split: 6, max_features: auto\n",
      "Accuracy Score: 0.99212271973466\n",
      "n_estimators: 100, max_depth: 10, learning_rate: 0.5, min_samples_leaf: 1, min_samples_split: 10, max_features: auto\n",
      "Accuracy Score: 0.9925373134328358\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\Data Analytics\\Analysis Projects\\Final Project\\Final-Project\\Machine_Leanring_Notebooks\\predict_operating_margin_ML.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y111sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlog2\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y111sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     gbr \u001b[39m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[39m=\u001b[39mn, max_depth\u001b[39m=\u001b[39md, learning_rate\u001b[39m=\u001b[39ml, min_samples_leaf\u001b[39m=\u001b[39mm, min_samples_split\u001b[39m=\u001b[39ms, max_features\u001b[39m=\u001b[39mf)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y111sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     gbr\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y111sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     gbr_predictions \u001b[39m=\u001b[39m gbr\u001b[39m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y111sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     gb_acc_score_in_loop \u001b[39m=\u001b[39m accuracy_score(y_test, gbr_predictions)\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:586\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resize_state()\n\u001b[0;32m    585\u001b[0m \u001b[39m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m n_stages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stages(\n\u001b[0;32m    587\u001b[0m     X,\n\u001b[0;32m    588\u001b[0m     y,\n\u001b[0;32m    589\u001b[0m     raw_predictions,\n\u001b[0;32m    590\u001b[0m     sample_weight,\n\u001b[0;32m    591\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rng,\n\u001b[0;32m    592\u001b[0m     X_val,\n\u001b[0;32m    593\u001b[0m     y_val,\n\u001b[0;32m    594\u001b[0m     sample_weight_val,\n\u001b[0;32m    595\u001b[0m     begin_at_stage,\n\u001b[0;32m    596\u001b[0m     monitor,\n\u001b[0;32m    597\u001b[0m )\n\u001b[0;32m    599\u001b[0m \u001b[39m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39mif\u001b[39;00m n_stages \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:663\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    656\u001b[0m     old_oob_score \u001b[39m=\u001b[39m loss_(\n\u001b[0;32m    657\u001b[0m         y[\u001b[39m~\u001b[39msample_mask],\n\u001b[0;32m    658\u001b[0m         raw_predictions[\u001b[39m~\u001b[39msample_mask],\n\u001b[0;32m    659\u001b[0m         sample_weight[\u001b[39m~\u001b[39msample_mask],\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stage(\n\u001b[0;32m    664\u001b[0m     i,\n\u001b[0;32m    665\u001b[0m     X,\n\u001b[0;32m    666\u001b[0m     y,\n\u001b[0;32m    667\u001b[0m     raw_predictions,\n\u001b[0;32m    668\u001b[0m     sample_weight,\n\u001b[0;32m    669\u001b[0m     sample_mask,\n\u001b[0;32m    670\u001b[0m     random_state,\n\u001b[0;32m    671\u001b[0m     X_csc,\n\u001b[0;32m    672\u001b[0m     X_csr,\n\u001b[0;32m    673\u001b[0m )\n\u001b[0;32m    675\u001b[0m \u001b[39m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:249\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    246\u001b[0m tree\u001b[39m.\u001b[39mfit(X, residual, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    248\u001b[0m \u001b[39m# update tree leaves\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m loss\u001b[39m.\u001b[39;49mupdate_terminal_regions(\n\u001b[0;32m    250\u001b[0m     tree\u001b[39m.\u001b[39;49mtree_,\n\u001b[0;32m    251\u001b[0m     X,\n\u001b[0;32m    252\u001b[0m     y,\n\u001b[0;32m    253\u001b[0m     residual,\n\u001b[0;32m    254\u001b[0m     raw_predictions,\n\u001b[0;32m    255\u001b[0m     sample_weight,\n\u001b[0;32m    256\u001b[0m     sample_mask,\n\u001b[0;32m    257\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[0;32m    258\u001b[0m     k\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m    259\u001b[0m )\n\u001b[0;32m    261\u001b[0m \u001b[39m# add tree to ensemble\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_[i, k] \u001b[39m=\u001b[39m tree\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py:121\u001b[0m, in \u001b[0;36mLossFunction.update_terminal_regions\u001b[1;34m(self, tree, X, y, residual, raw_predictions, sample_weight, sample_mask, learning_rate, k)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m# update each leaf (= perform line search)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m leaf \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mwhere(tree\u001b[39m.\u001b[39mchildren_left \u001b[39m==\u001b[39m TREE_LEAF)[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_terminal_region(\n\u001b[0;32m    122\u001b[0m         tree,\n\u001b[0;32m    123\u001b[0m         masked_terminal_regions,\n\u001b[0;32m    124\u001b[0m         leaf,\n\u001b[0;32m    125\u001b[0m         X,\n\u001b[0;32m    126\u001b[0m         y,\n\u001b[0;32m    127\u001b[0m         residual,\n\u001b[0;32m    128\u001b[0m         raw_predictions[:, k],\n\u001b[0;32m    129\u001b[0m         sample_weight,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    132\u001b[0m \u001b[39m# update predictions (both in-bag and out-of-bag)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m raw_predictions[:, k] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m tree\u001b[39m.\u001b[39mvalue[:, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtake(\n\u001b[0;32m    134\u001b[0m     terminal_regions, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m    135\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py:724\u001b[0m, in \u001b[0;36mBinomialDeviance._update_terminal_region\u001b[1;34m(self, tree, terminal_regions, leaf, X, y, residual, raw_predictions, sample_weight)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_terminal_region\u001b[39m(\n\u001b[0;32m    706\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    707\u001b[0m     tree,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    714\u001b[0m     sample_weight,\n\u001b[0;32m    715\u001b[0m ):\n\u001b[0;32m    716\u001b[0m     \u001b[39m\"\"\"Make a single Newton-Raphson step.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \n\u001b[0;32m    718\u001b[0m \u001b[39m    our node estimate is given by:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[39m    we take advantage that: y - prob = residual\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 724\u001b[0m     terminal_region \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(terminal_regions \u001b[39m==\u001b[39;49m leaf)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    725\u001b[0m     residual \u001b[39m=\u001b[39m residual\u001b[39m.\u001b[39mtake(terminal_region, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    726\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mtake(terminal_region, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Use the past accuracy score to find the best parameters\n",
    "gbr_acc_score_loop = 0.9593\n",
    "\n",
    "#Use for loop to find optimal parameters\n",
    "for n in [100, 300, 500, 700]:\n",
    "    for d in [2, 6, 10, 14]:\n",
    "        for l in [0.01, 0.1, 0.5, 0.75]:\n",
    "            for m in [1, 3, 5, 7]:\n",
    "                for s in [2, 6, 10, 14]:\n",
    "                    for f in ['auto', 'sqrt', 'log2']:\n",
    "                        gbr = GradientBoostingClassifier(n_estimators=n, max_depth=d, learning_rate=l, min_samples_leaf=m, min_samples_split=s, max_features=f)\n",
    "                        gbr.fit(X_train_scaled, y_train)\n",
    "                        gbr_predictions = gbr.predict(X_test_scaled)\n",
    "                        gb_acc_score_in_loop = accuracy_score(y_test, gbr_predictions)\n",
    "                        \n",
    "                        #If the accuracy score is better than the previous one, print the parameters and accuracy score\n",
    "                        if gb_acc_score_in_loop > gbr_acc_score_loop:\n",
    "                            gbr_acc_score_loop = gb_acc_score_in_loop\n",
    "                            print(f'n_estimators: {n}, max_depth: {d}, learning_rate: {l}, min_samples_leaf: {m}, min_samples_split: {s}, max_features: {f}')\n",
    "                            print(f'Accuracy Score: {gb_acc_score_in_loop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I ran the above for loop for 10 minutes, while it did not yet even get to the second parameter in the first for loop,\n",
    "#it has already achieved 0.992 accuracy score, which is more than good enough for the purpose of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Gradient Boosting Classifier instance\n",
    "opt_gbr = GradientBoostingClassifier(learning_rate=0.5, max_depth=10, max_features='auto', min_samples_leaf=1, min_samples_split=10, n_estimators=100)\n",
    "\n",
    "#Fit the model\n",
    "opt_gbr = opt_gbr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Make predictions\n",
    "opt_gbr_predictions = opt_gbr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized GBR Confusion Matrix:\n",
      "           Predicted 0  Predicted 1\n",
      "Actual 0          375           20\n",
      "Actual 1            5         2012\n",
      "Original GBR Confusion Matrix:\n",
      "           Predicted 0  Predicted 1\n",
      "Actual 0          309           86\n",
      "Actual 1           12         2005\n"
     ]
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "opt_gbr_cm = confusion_matrix(y_test, opt_gbr_predictions)\n",
    "\n",
    "#Create a DataFrame from the confusion matrix\n",
    "opt_gbr_cm_df = pd.DataFrame(opt_gbr_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "print(f'Optimized GBR Confusion Matrix:\\n', opt_gbr_cm_df)\n",
    "print(f'Original GBR Confusion Matrix:\\n', gbr_cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized GBR Accuracy Score: 0.9896351575456053\n",
      "Original GBR Accuracy Score: 0.9593698175787728\n"
     ]
    }
   ],
   "source": [
    "#Calculate the accuracy score\n",
    "opt_gbr_acc_score = accuracy_score(y_test, opt_gbr_predictions)\n",
    "\n",
    "print(f'Optimized GBR Accuracy Score: {opt_gbr_acc_score}')\n",
    "print(f'Original GBR Accuracy Score: {gb_acc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized GBR Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       395\n",
      "           1       0.99      1.00      0.99      2017\n",
      "\n",
      "    accuracy                           0.99      2412\n",
      "   macro avg       0.99      0.97      0.98      2412\n",
      "weighted avg       0.99      0.99      0.99      2412\n",
      "\n",
      "Original GBR Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82       395\n",
      "           1       0.95      0.99      0.97      2017\n",
      "\n",
      "    accuracy                           0.95      2412\n",
      "   macro avg       0.95      0.85      0.89      2412\n",
      "weighted avg       0.95      0.95      0.95      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the classification report\n",
    "print(f'Optimized GBR Report:\\n', classification_report(y_test, opt_gbr_predictions))\n",
    "print(f'Original GBR Report:\\n', classification_report(y_test, gbr_predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTEENN - Synthetic Minority Oversampling Technique and Edited Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mitch\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Create a SMOTEENN instance\n",
    "smote_enn = SMOTEENN()\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "#Use logistic regression to create model\n",
    "smoteenn_model = LogisticRegression()\n",
    "smoteenn_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "#Make predictions\n",
    "smoteenn_predictions = smoteenn_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>384</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>109</td>\n",
       "      <td>1908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          384           11\n",
       "Actual 1          109         1908"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "smoteenn_cm = confusion_matrix(y_test, smoteenn_predictions)\n",
    "\n",
    "#Create a DataFrame from the confusion matrix\n",
    "smoteenn_cm_df = pd.DataFrame(smoteenn_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "smoteenn_cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9502487562189055"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the accuracy score\n",
    "smoteenn_acc_score = accuracy_score(y_test, smoteenn_predictions)\n",
    "smoteenn_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.86       395\n",
      "           1       0.99      0.95      0.97      2017\n",
      "\n",
      "    accuracy                           0.95      2412\n",
      "   macro avg       0.89      0.96      0.92      2412\n",
      "weighted avg       0.96      0.95      0.95      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the classification report\n",
    "print(classification_report(y_test, smoteenn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create grid search parameters for smoteenn\n",
    "smote_grid = {'n_estimators': [100, 300, 500, 700],\n",
    "                'max_depth': [2, 6, 10, 14],\n",
    "                'min_samples_leaf': [1, 3, 5, 7],\n",
    "                'min_samples_split': [2, 6, 10, 14],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                'learning_rate': [0.01, 0.1, 0.5, 0.75]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the SMOTEENN model using for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems that the for loop may be working better than the GridSearchCV, to save some time I will only use the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\Data Analytics\\Analysis Projects\\Final Project\\Final-Project\\Machine_Leanring_Notebooks\\predict_operating_margin_ML.ipynb Cell 62\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y123sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m [\u001b[39m2\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m14\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y123sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlog2\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y123sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m#Use logistic regression to create model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y123sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         smoteenn_model \u001b[39m=\u001b[39m LogisticRegression(solver\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlbfgs\u001b[39;49m\u001b[39m'\u001b[39;49m, max_iter\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, max_depth\u001b[39m=\u001b[39;49md, learning_rate\u001b[39m=\u001b[39;49ml, min_samples_leaf\u001b[39m=\u001b[39;49mm, min_samples_split\u001b[39m=\u001b[39;49ms, max_features\u001b[39m=\u001b[39;49mf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y123sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         smoteenn_model\u001b[39m.\u001b[39mfit(X_resampled, y_resampled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Data%20Analytics/Analysis%20Projects/Final%20Project/Final-Project/Machine_Leanring_Notebooks/predict_operating_margin_ML.ipynb#Y123sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m#Make predictions\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_depth'"
     ]
    }
   ],
   "source": [
    "#Use the past accuracy score to find the best parameters\n",
    "smoteenn_acc_score_loop = 0.9593\n",
    "\n",
    "for n in [100, 300, 500, 700]:\n",
    "    for d in [2, 6, 10, 14]:\n",
    "        for l in [0.01, 0.1, 0.5, 0.75]:\n",
    "            for m in [1, 3, 5, 7]:\n",
    "                for s in [2, 6, 10, 14]:\n",
    "                    for f in ['auto', 'sqrt', 'log2']:\n",
    "                        #Use logistic regression to create model\n",
    "                        smoteenn_model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "                        smoteenn_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "                        #Make predictions\n",
    "                        smoteenn_predictions = smoteenn_model.predict(X_test_scaled)\n",
    "                        \n",
    "                        #Calculate the accuracy score\n",
    "                        smoteenn_acc_score_in_loop = accuracy_score(y_test, smoteenn_predictions)\n",
    "                        \n",
    "                        if smoteenn_acc_score_in_loop > smoteenn_acc_score_loop:\n",
    "                            smoteenn_acc_score_loop = smoteenn_acc_score_in_loop\n",
    "                            print(f'n_estimators: {n}, max_depth: {d}, learning_rate: {l}, min_samples_leaf: {m}, min_samples_split: {s}, max_features: {f}')\n",
    "                            print(f'Accuracy Score: {smoteenn_acc_score_in_loop}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "335ee12212264728feb72f243af72c5a8ea26c832f07e1f651ce9e17c7ceae23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
